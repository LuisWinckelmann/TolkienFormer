<!-- README.md -->
<!-- Project Top -->
<a name="readme-top"></a>

<h1 align="center">TolkienFormer - Text generation with Tolkien's Touch</h1>
  <p align="center">
    A project exploring LSTM and Transformer-like models to generate text with implementations in Python & Pytorch.
  <br />
  <!-- <a href="#results"><strong>Example Results Â»</strong></a> -->
</p>


<!-- TABLE OF CONTENTS -->
<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href="#about-the-project">About The Project</a></li>
    <li><a href="#setup">Setup</a></li>
    <li><a href="#data">Data</a></li>
    <li><a href="#training">Training</a></li>
    <li><a href="#testing">Testing</a></li>
    <li><a href="#roadmap">Roadmap</a></li>
    <li><a href="#license">License</a></li>
    <li><a href="#contact">Contact</a></li>
  </ol>
</details>


<!-- ABOUT THE PROJECT -->
## About The Project
Welcome to TolkienFormer, a personal project that dives into the task of text generation. 
This project explores LSTMs and Transformer-like models to generate text reminiscent of J.R.R. Tolkien's *The Lord of the Rings*.
While aiming to produce reasonable results, the primary goal is not to achieve state-of-the-art results but instead to enhance proficiency with Transformers, LSTMs, and PyTorch.

Text generation poses significant challenges in terms of data and computational resources. Thus, TolkienFormer employs the technique of [Teacher Forcing](https://en.wikipedia.org/wiki/Teacher_forcing) to stabilize and expedite training and testing. 

An example of the output generated by TolkienFormer after training and fine-tuning a model can be seen below:
[![Example Text Production of a trained LSTM][fitted-lstm]](gfx/fitted_lstm.png)

<!--
Since Text generation poses as a particularly demanding task in terms of data and computational resources, this project does not aim to produce state-of-the-art results, but instead the goal was to reach reasonable results while  enhancing proficiency with Transformers, LSTMs, and PyTorch in a broader sense. <br>
Given the relatively small size of both the models and datasets, the project employs the technique of [Teacher Forcing](https://en.wikipedia.org/wiki/Teacher_forcing).
This method provides the model with the actual ground truth output from the previous step as input, rather than its own generated output, helping to stabilize and expedite training and testing.

With the Initialization via Teacher Forcing, here is an example result when running `test.py` after training, and finetuning a model:
[![Example Text Production of a trained LSTM][fitted-lstm]](gfx/fitted_lstm.png)
-------------
Text generation is considered a particular data and resource hungry task. 
In order to reduce the complexity of the problem of text generation and to work with the limited computational capacity of my personal GPU the results are obtained with a rather small dataset (Chapter 1 of Tolkien's The Fellowship of the Ring).
To counteract the limited size of the models and the dataset, [Teacher Forcing](https://en.wikipedia.org/wiki/Teacher_forcing) was used to pretend that the model correctly predicted the first *n* characters.
When the hyperparameters are tuned correctly, the trained models are able to produce the following texts:
-->

## Setup 
  First, you have to clone the repo and create a conda environment, as well adding the project root to your PYTHONPATH to enable local imports:
   ```shell
   # 1. Clone this repository
   git clone https://github.com/LuisWinckelmann/TolkienFormer.git
   cd TolkienFormer
   # 2. Setup conda env
   conda create --name tolkienformer
   conda activate tolkienformer
   conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia
   # 3. Enable local imports by adding the root to your pythonpath:
   # 3a) Linux:
   export PYTHONPATH=$PYTHONPATH:$PWD
   # 3b) Windows:
   set PYTHONPATH=%PYTHONPATH%;%cd%
   ```
  Afterwards, you need prepare the data to be able to train the models. For the specifics, please follow the [Instructions](#data) below. 

## Data
  For the data you can use any *.txt file that you want. In the current setup the file will get parse row-wise.
  The example dataset chapter1, provided in `src/data/chapter1` includes chapter 1 or Tolkien's *The Fellowship of the Ring* obtained from [here](https://ae-lib.org.ua/texts-c/tolkien__the_lord_of_the_rings_3__en.htm).
  To use your own dataset simply copy the text file(s) into `src/data` and run:
  ```shell
  cd src/data
  python data_preparation.py 
  ```
  If your data is stored somewhere else then `src/data` you can use `--path_to_folder_with_txt_filess` to adjust the root folder with the .txt files inside.
  If your data has another format you'll need to adjust your custom dataset in `src/utils/datasets.py` accordingly.

## Training
  To run training of the LSTM run:
  ```shell
  cd src/models/lstm
  python train.py 
  ```
  To run training of the transformer-like model run:
  ```shell
  cd src/models/transformer
  python train.py 
  ```
  All currently available hyperparameters can be changed in the corresponding config.json files located in `src/modes/lstm` or `src/modes/transformers` respectively. 

## Testing
  After executing the training, to generate results of the models as shown in the <a href="#about-the-project">description</a>, you can run:
  ```shell
  # LSTM model
  cd src/models/lstm
  python test.py 
  # Transformer-like model
  cd src/models/transformer
  python test.py 
  # Optional Parameters to edit when running test.py: 
  # --num_sentences 5 
  # --model_epoch 150
  ```
  To specify the amount of predicted sentences use the `--num_sentences` flag, to select one of the saved checkpoints, use the `--model_epoch` flag
  Other parameters for the evaluation can be changed in the model `config.json`.

## Roadmap
- [ ] Move to logging from printing
- [ ] Write description with a showcase
- [ ] Publish some additional results
- [ ] Confirm setup and functionality works and README is clearly written
- [ ] Get rid of code doubling my merging LSTM & Transformer folders and specifically train.py & test.py 
- [ ] Easier setup via shell script(s)


## License
Distributed under the MIT License. See `LICENSE.txt` for more information.

## Contact
[![LinkedIn][linkedin-shield]][linkedin-url] <br>
Luis Winckelmann  - luis.winckelmann@gmx.com <br>
Project Link: [https://github.com/LuisWinckelmann/TolkienFormer](https://github.com/LuisWinckelmann/TolkienFormer)

<p align="right">(<a href="#readme-top">back to top</a>)</p>

[license-shield]: https://img.shields.io/github/license/LuisWinckelmann/JavaDeep-MLP-RNN-from-scratch-in-Java.svg?style=for-the-badge
[license-url]: https://github.com/LuisWinckelmann/JavaDeep-MLP-RNN-from-scratch-in-Java/blob/main/LICENSE.txt
[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=for-the-badge&logo=linkedin&colorB=555
[linkedin-url]: https://linkedin.com/in/luiswinckelmann
[PyTorch]: https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=for-the-badge&logo=PyTorch&logoColor=white
[fitted-lstm]: gfx/fitted_lstm.png
<!--
[underfitted-results-transformer]: gfx/underfitted_transformer.png
[overfitted-results-transformer]: gfx/overfitted_transformer.png
[underfitted-results-lstm]: gfx/underfitted_lstm.png
[overfitted-results-lstm]: gfx/overfitted_lstm.png
[fitted-results-transformer]: gfx/fitted_transformer.png
-->
